{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHQItaAn+T4tBDap6FVvea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gavinchen104/BERT_classification/blob/main/BERT1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xd2wE_DavgQQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {\n",
        "  'R': 0,\n",
        "  'java': 1,\n",
        "  'javascript': 2,\n",
        "  'php': 3,\n",
        "  'python': 4\n",
        "}\n",
        "\n",
        "def load_data(data_file):\n",
        "  df = pd.read_csv(data_file, sep=';')\n",
        "\n",
        "  # Apply the custom label mapping to the 'label' column\n",
        "  df['category'] = df['label'].map(label_mapping)\n",
        "  df_filtered = df.groupby('category').apply(lambda x: x.sample(n=400, replace=True if len(x) < 300 else False)).reset_index(drop=True)\n",
        "\n",
        "  titles = df_filtered['title'].tolist()\n",
        "  labels = df_filtered['category'].tolist()\n",
        "\n",
        "  return titles, labels"
      ],
      "metadata": {
        "id": "O4Kb4_WdvpfF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles, labels = load_data('/content/full_dataset_v3.csv')"
      ],
      "metadata": {
        "id": "-yxaRgkdwjoR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "  def __init__(self, text, label, tokenizer, max_length):\n",
        "    self.text = text\n",
        "    self.label = label\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.text[idx]\n",
        "    label = self.label[idx]\n",
        "    encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "    return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
      ],
      "metadata": {
        "id": "9Q2D1ZMzyC9t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self, bert_model_name, num_classes):\n",
        "    super(BERTClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    pooled_output = outputs.pooler_output\n",
        "    x = self.dropout(pooled_output)\n",
        "    logits = self.fc(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "0VAkruFzyWAX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, optimizer, scheduler, device):\n",
        "  model.train()\n",
        "  for batch in data_loader:\n",
        "    optimizer.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "fhkbGhe1yaPH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actual_labels = []\n",
        "  with torch.no_grad():\n",
        "      for batch in data_loader:\n",
        "          input_ids = batch['input_ids'].to(device)\n",
        "          attention_mask = batch['attention_mask'].to(device)\n",
        "          labels = batch['label'].to(device)\n",
        "          outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "          _, preds = torch.max(outputs, dim=1)\n",
        "          predictions.extend(preds.cpu().tolist())\n",
        "          actual_labels.extend(labels.cpu().tolist())\n",
        "  return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions, zero_division=1)"
      ],
      "metadata": {
        "id": "ZxCQwvD2yeZi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Reverse mapping from numerical category back to label\n",
        "    reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "    return reverse_label_mapping.get(preds, \"Unknown\")"
      ],
      "metadata": {
        "id": "o9j4yASHyhmC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up parameters\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "num_classes = 5\n",
        "max_length = 256\n",
        "batch_size = 20\n",
        "num_epochs = 4\n",
        "learning_rate = 2e-5"
      ],
      "metadata": {
        "id": "PB0YrLqr0BeT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(titles, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "muve-f_N0Oxk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = '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'\n",
        "token = os.getenv(\"HF_TOKEN\")\n"
      ],
      "metadata": {
        "id": "k_s6qdQYOQmR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "pWILfcy50YfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6486dd3-1429-4884-a06b-ae5f06b2e8fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bert_model_name, num_classes).to(device)"
      ],
      "metadata": {
        "id": "C1nIVImW0gRC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ],
      "metadata": {
        "id": "bm3VY6QM0jgK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "  train(model, train_dataloader, optimizer, scheduler, device)\n",
        "  accuracy, report = evaluate(model, val_dataloader, device)\n",
        "  print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "  print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF_vO4BZ1B3y",
        "outputId": "a9d01cf3-0fa2-4392-fa2a-4e83cd7d7ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"bert_classifier.pth\")\n",
        "test_text = \"Package python software with pylucene dependency\"\n",
        "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
        "print(f\"Predicted sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "q8R3xtZB14Vv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}